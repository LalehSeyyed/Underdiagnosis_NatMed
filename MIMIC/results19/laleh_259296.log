Fri Dec  6 12:19:27 EST 2019
guppy9
/h/laleh/PycharmProjects/Fairness/Nov28/MIMIC/5
  0%|          | 0/65 [00:00<?, ?it/s]Validation_df size: 37300
Train_df size 298137
Test_df size 36421
Validation_df path 37300
Train_df path 298137
Epoch 0/64
----------
0
24000
48000
72000
96000
120000
144000
168000
192000
216000
240000
264000
288000
Train_losses: [0.2485448569059372]
  2%|▏         | 1/65 [2:10:35<139:17:24, 7835.07s/it]0
24000
Validation_losses: [0.2430315613746643]
saving
Epoch 1/64
----------
0
24000
48000
72000
96000
120000
144000
168000
192000
216000
240000
264000
288000
Train_losses: [0.2485448569059372, 0.23860414326190948]
  3%|▎         | 2/65 [4:27:02<138:57:40, 7940.64s/it]0
24000
Validation_losses: [0.2430315613746643, 0.23668794333934784]
saving
Epoch 2/64
----------
0
24000
48000
72000
96000
120000
144000
168000
192000
216000
240000
264000
288000
Train_losses: [0.2485448569059372, 0.23860414326190948, 0.23417140543460846]
  5%|▍         | 3/65 [6:51:51<140:37:33, 8165.37s/it]0
24000
Validation_losses: [0.2430315613746643, 0.23668794333934784, 0.23673632740974426]
Epoch 3/64
----------
0
24000
48000
72000
96000
120000
144000
168000
192000
216000
240000
264000
288000
Train_losses: [0.2485448569059372, 0.23860414326190948, 0.23417140543460846, 0.23109398782253265]
  6%|▌         | 4/65 [9:12:15<139:40:24, 8243.02s/it]0
24000
Validation_losses: [0.2430315613746643, 0.23668794333934784, 0.23673632740974426, 0.23185330629348755]
saving
Epoch 4/64
----------
0
24000
48000
72000
96000
120000
144000
168000
192000
216000
240000
264000
288000
Train_losses: [0.2485448569059372, 0.23860414326190948, 0.23417140543460846, 0.23109398782253265, 0.2286403626203537]
  8%|▊         | 5/65 [11:39:46<140:25:24, 8425.42s/it]0
24000
Validation_losses: [0.2430315613746643, 0.23668794333934784, 0.23673632740974426, 0.23185330629348755, 0.23108941316604614]
saving
Epoch 5/64
----------
0
24000
48000
72000
96000
120000
144000
168000
192000
216000
240000
264000
288000
Train_losses: [0.2485448569059372, 0.23860414326190948, 0.23417140543460846, 0.23109398782253265, 0.2286403626203537, 0.2266765534877777]
  9%|▉         | 6/65 [14:07:23<140:12:08, 8554.71s/it]0
24000
Validation_losses: [0.2430315613746643, 0.23668794333934784, 0.23673632740974426, 0.23185330629348755, 0.23108941316604614, 0.23243823647499084]
Epoch 6/64
----------
0
24000
48000
72000
96000
120000
144000
168000
192000
216000
240000
264000
288000
Train_losses: [0.2485448569059372, 0.23860414326190948, 0.23417140543460846, 0.23109398782253265, 0.2286403626203537, 0.2266765534877777, 0.22503556311130524]
 11%|█         | 7/65 [16:36:38<139:45:32, 8674.69s/it]0
24000
Validation_losses: [0.2430315613746643, 0.23668794333934784, 0.23673632740974426, 0.23185330629348755, 0.23108941316604614, 0.23243823647499084, 0.22902993857860565]
saving
Epoch 7/64
----------
0
24000
48000
72000
96000
120000
144000
168000
192000
216000
240000
264000
288000
Train_losses: [0.2485448569059372, 0.23860414326190948, 0.23417140543460846, 0.23109398782253265, 0.2286403626203537, 0.2266765534877777, 0.22503556311130524, 0.22336895763874054]
 12%|█▏        | 8/65 [19:04:03<138:09:42, 8726.01s/it]0
24000
Validation_losses: [0.2430315613746643, 0.23668794333934784, 0.23673632740974426, 0.23185330629348755, 0.23108941316604614, 0.23243823647499084, 0.22902993857860565, 0.2607364356517792]
Epoch 8/64
----------
0
24000
48000
72000
96000
120000
144000
168000
192000
216000
240000
264000
288000
Train_losses: [0.2485448569059372, 0.23860414326190948, 0.23417140543460846, 0.23109398782253265, 0.2286403626203537, 0.2266765534877777, 0.22503556311130524, 0.22336895763874054, 0.22203819453716278]
 14%|█▍        | 9/65 [21:29:51<135:50:17, 8732.45s/it]0
24000
Validation_losses: [0.2430315613746643, 0.23668794333934784, 0.23673632740974426, 0.23185330629348755, 0.23108941316604614, 0.23243823647499084, 0.22902993857860565, 0.2607364356517792, 0.23197954893112183]
Epoch 9/64
----------
0
24000
48000
72000
96000
120000
144000
168000
192000
216000
240000
264000
288000
Train_losses: [0.2485448569059372, 0.23860414326190948, 0.23417140543460846, 0.23109398782253265, 0.2286403626203537, 0.2266765534877777, 0.22503556311130524, 0.22336895763874054, 0.22203819453716278, 0.22045648097991943]
 15%|█▌        | 10/65 [23:55:33<133:27:23, 8735.33s/it]0
24000
Validation_losses: [0.2430315613746643, 0.23668794333934784, 0.23673632740974426, 0.23185330629348755, 0.23108941316604614, 0.23243823647499084, 0.22902993857860565, 0.2607364356517792, 0.23197954893112183, 0.22878891229629517]
saving
Epoch 10/64
----------
0
24000
48000
72000
96000
120000
144000
168000
192000
216000
240000
264000
288000
Train_losses: [0.2485448569059372, 0.23860414326190948, 0.23417140543460846, 0.23109398782253265, 0.2286403626203537, 0.2266765534877777, 0.22503556311130524, 0.22336895763874054, 0.22203819453716278, 0.22045648097991943, 0.21903084218502045]
 17%|█▋        | 11/65 [26:20:51<130:57:15, 8730.28s/it]0
24000
Validation_losses: [0.2430315613746643, 0.23668794333934784, 0.23673632740974426, 0.23185330629348755, 0.23108941316604614, 0.23243823647499084, 0.22902993857860565, 0.2607364356517792, 0.23197954893112183, 0.22878891229629517, 0.23141248524188995]
Epoch 11/64
----------
0
24000
48000
72000
96000
120000
144000
168000
192000
216000
240000
264000
288000
Train_losses: [0.2485448569059372, 0.23860414326190948, 0.23417140543460846, 0.23109398782253265, 0.2286403626203537, 0.2266765534877777, 0.22503556311130524, 0.22336895763874054, 0.22203819453716278, 0.22045648097991943, 0.21903084218502045, 0.21764634549617767]
 18%|█▊        | 12/65 [28:46:50<128:39:22, 8738.92s/it]0
24000
Validation_losses: [0.2430315613746643, 0.23668794333934784, 0.23673632740974426, 0.23185330629348755, 0.23108941316604614, 0.23243823647499084, 0.22902993857860565, 0.2607364356517792, 0.23197954893112183, 0.22878891229629517, 0.23141248524188995, 0.22894157469272614]
Epoch 12/64
----------
0
24000
48000
72000
96000
120000
144000
168000
192000
216000
240000
264000
288000
Train_losses: [0.2485448569059372, 0.23860414326190948, 0.23417140543460846, 0.23109398782253265, 0.2286403626203537, 0.2266765534877777, 0.22503556311130524, 0.22336895763874054, 0.22203819453716278, 0.22045648097991943, 0.21903084218502045, 0.21764634549617767, 0.21622450649738312]
 20%|██        | 13/65 [31:10:12<125:38:01, 8697.71s/it]0
24000
Validation_losses: [0.2430315613746643, 0.23668794333934784, 0.23673632740974426, 0.23185330629348755, 0.23108941316604614, 0.23243823647499084, 0.22902993857860565, 0.2607364356517792, 0.23197954893112183, 0.22878891229629517, 0.23141248524188995, 0.22894157469272614, 0.2295830398797989]
decay loss from 0.0005 to 0.00025 as not seeing improvement in val loss
created new optimizer with LR 0.00025
Epoch 13/64
----------
0
24000
48000
72000
96000
120000
144000
168000
192000
216000
240000
264000
288000
Train_losses: [0.2485448569059372, 0.23860414326190948, 0.23417140543460846, 0.23109398782253265, 0.2286403626203537, 0.2266765534877777, 0.22503556311130524, 0.22336895763874054, 0.22203819453716278, 0.22045648097991943, 0.21903084218502045, 0.21764634549617767, 0.21622450649738312, 0.20988605916500092]
 22%|██▏       | 14/65 [33:33:58<122:54:52, 8676.32s/it]0
24000
Validation_losses: [0.2430315613746643, 0.23668794333934784, 0.23673632740974426, 0.23185330629348755, 0.23108941316604614, 0.23243823647499084, 0.22902993857860565, 0.2607364356517792, 0.23197954893112183, 0.22878891229629517, 0.23141248524188995, 0.22894157469272614, 0.2295830398797989, 0.22708526253700256]
saving
Epoch 14/64
----------
0
24000
48000
72000
96000
120000
144000
168000
192000
216000
240000
264000
288000
Train_losses: [0.2485448569059372, 0.23860414326190948, 0.23417140543460846, 0.23109398782253265, 0.2286403626203537, 0.2266765534877777, 0.22503556311130524, 0.22336895763874054, 0.22203819453716278, 0.22045648097991943, 0.21903084218502045, 0.21764634549617767, 0.21622450649738312, 0.20988605916500092, 0.20717333257198334]
 23%|██▎       | 15/65 [35:58:13<120:24:52, 8669.85s/it]0
24000
Validation_losses: [0.2430315613746643, 0.23668794333934784, 0.23673632740974426, 0.23185330629348755, 0.23108941316604614, 0.23243823647499084, 0.22902993857860565, 0.2607364356517792, 0.23197954893112183, 0.22878891229629517, 0.23141248524188995, 0.22894157469272614, 0.2295830398797989, 0.22708526253700256, 0.2305670976638794]
Epoch 15/64
----------
0
24000
48000
72000
96000
120000
144000
168000
192000
216000
240000
264000
288000
Train_losses: [0.2485448569059372, 0.23860414326190948, 0.23417140543460846, 0.23109398782253265, 0.2286403626203537, 0.2266765534877777, 0.22503556311130524, 0.22336895763874054, 0.22203819453716278, 0.22045648097991943, 0.21903084218502045, 0.21764634549617767, 0.21622450649738312, 0.20988605916500092, 0.20717333257198334, 0.20503060519695282]
 25%|██▍       | 16/65 [38:23:01<118:04:40, 8675.11s/it]0
24000
Validation_losses: [0.2430315613746643, 0.23668794333934784, 0.23673632740974426, 0.23185330629348755, 0.23108941316604614, 0.23243823647499084, 0.22902993857860565, 0.2607364356517792, 0.23197954893112183, 0.22878891229629517, 0.23141248524188995, 0.22894157469272614, 0.2295830398797989, 0.22708526253700256, 0.2305670976638794, 0.23122388124465942]
Epoch 16/64
----------
0
24000
48000
72000
96000
120000
144000
168000
192000
216000
240000
264000
288000
Train_losses: [0.2485448569059372, 0.23860414326190948, 0.23417140543460846, 0.23109398782253265, 0.2286403626203537, 0.2266765534877777, 0.22503556311130524, 0.22336895763874054, 0.22203819453716278, 0.22045648097991943, 0.21903084218502045, 0.21764634549617767, 0.21622450649738312, 0.20988605916500092, 0.20717333257198334, 0.20503060519695282, 0.2030644416809082]
 26%|██▌       | 17/65 [40:47:26<115:37:46, 8672.22s/it]0
24000
Validation_losses: [0.2430315613746643, 0.23668794333934784, 0.23673632740974426, 0.23185330629348755, 0.23108941316604614, 0.23243823647499084, 0.22902993857860565, 0.2607364356517792, 0.23197954893112183, 0.22878891229629517, 0.23141248524188995, 0.22894157469272614, 0.2295830398797989, 0.22708526253700256, 0.2305670976638794, 0.23122388124465942, 0.23493289947509766]
decay loss from 0.00025 to 0.000125 as not seeing improvement in val loss
created new optimizer with LR 0.000125
Epoch 17/64
----------
0
24000
48000
72000
96000
120000
144000
168000
192000
216000
240000
264000
288000
Train_losses: [0.2485448569059372, 0.23860414326190948, 0.23417140543460846, 0.23109398782253265, 0.2286403626203537, 0.2266765534877777, 0.22503556311130524, 0.22336895763874054, 0.22203819453716278, 0.22045648097991943, 0.21903084218502045, 0.21764634549617767, 0.21622450649738312, 0.20988605916500092, 0.20717333257198334, 0.20503060519695282, 0.2030644416809082, 0.1974671334028244]
 28%|██▊       | 18/65 [43:12:15<113:17:07, 8677.18s/it]0
24000
Validation_losses: [0.2430315613746643, 0.23668794333934784, 0.23673632740974426, 0.23185330629348755, 0.23108941316604614, 0.23243823647499084, 0.22902993857860565, 0.2607364356517792, 0.23197954893112183, 0.22878891229629517, 0.23141248524188995, 0.22894157469272614, 0.2295830398797989, 0.22708526253700256, 0.2305670976638794, 0.23122388124465942, 0.23493289947509766, 0.23303672671318054]
decay loss from 0.000125 to 6.25e-05 as not seeing improvement in val loss
created new optimizer with LR 6.25e-05
Epoch 18/64
----------
0
24000
48000
72000
96000
120000
144000
168000
192000
216000
240000
264000
288000
Train_losses: [0.2485448569059372, 0.23860414326190948, 0.23417140543460846, 0.23109398782253265, 0.2286403626203537, 0.2266765534877777, 0.22503556311130524, 0.22336895763874054, 0.22203819453716278, 0.22045648097991943, 0.21903084218502045, 0.21764634549617767, 0.21622450649738312, 0.20988605916500092, 0.20717333257198334, 0.20503060519695282, 0.2030644416809082, 0.1974671334028244, 0.19292093813419342]
 29%|██▉       | 19/65 [45:36:46<110:51:06, 8675.36s/it]0
24000
Validation_losses: [0.2430315613746643, 0.23668794333934784, 0.23673632740974426, 0.23185330629348755, 0.23108941316604614, 0.23243823647499084, 0.22902993857860565, 0.2607364356517792, 0.23197954893112183, 0.22878891229629517, 0.23141248524188995, 0.22894157469272614, 0.2295830398797989, 0.22708526253700256, 0.2305670976638794, 0.23122388124465942, 0.23493289947509766, 0.23303672671318054, 0.2354302704334259]
decay loss from 6.25e-05 to 3.125e-05 as not seeing improvement in val loss
created new optimizer with LR 3.125e-05
Epoch 19/64
----------
0
24000
48000
72000
96000
120000
144000
168000
192000
216000
240000
264000
288000
Train_losses: [0.2485448569059372, 0.23860414326190948, 0.23417140543460846, 0.23109398782253265, 0.2286403626203537, 0.2266765534877777, 0.22503556311130524, 0.22336895763874054, 0.22203819453716278, 0.22045648097991943, 0.21903084218502045, 0.21764634549617767, 0.21622450649738312, 0.20988605916500092, 0.20717333257198334, 0.20503060519695282, 0.2030644416809082, 0.1974671334028244, 0.19292093813419342, 0.19036108255386353]
 31%|███       | 20/65 [48:03:17<108:52:40, 8710.24s/it]0
24000
Validation_losses: [0.2430315613746643, 0.23668794333934784, 0.23673632740974426, 0.23185330629348755, 0.23108941316604614, 0.23243823647499084, 0.22902993857860565, 0.2607364356517792, 0.23197954893112183, 0.22878891229629517, 0.23141248524188995, 0.22894157469272614, 0.2295830398797989, 0.22708526253700256, 0.2305670976638794, 0.23122388124465942, 0.23493289947509766, 0.23303672671318054, 0.2354302704334259, 0.23636792600154877]
decay loss from 3.125e-05 to 1.5625e-05 as not seeing improvement in val loss
created new optimizer with LR 1.5625e-05
Epoch 20/64
----------
0
24000
48000
72000
96000
120000
144000
168000
192000
216000
240000
264000
288000
Train_losses: [0.2485448569059372, 0.23860414326190948, 0.23417140543460846, 0.23109398782253265, 0.2286403626203537, 0.2266765534877777, 0.22503556311130524, 0.22336895763874054, 0.22203819453716278, 0.22045648097991943, 0.21903084218502045, 0.21764634549617767, 0.21622450649738312, 0.20988605916500092, 0.20717333257198334, 0.20503060519695282, 0.2030644416809082, 0.1974671334028244, 0.19292093813419342, 0.19036108255386353, 0.18897560238838196]
 32%|███▏      | 21/65 [50:34:22<107:45:22, 8816.43s/it]0
24000
Validation_losses: [0.2430315613746643, 0.23668794333934784, 0.23673632740974426, 0.23185330629348755, 0.23108941316604614, 0.23243823647499084, 0.22902993857860565, 0.2607364356517792, 0.23197954893112183, 0.22878891229629517, 0.23141248524188995, 0.22894157469272614, 0.2295830398797989, 0.22708526253700256, 0.2305670976638794, 0.23122388124465942, 0.23493289947509766, 0.23303672671318054, 0.2354302704334259, 0.23636792600154877, 0.23725992441177368]
decay loss from 1.5625e-05 to 7.8125e-06 as not seeing improvement in val loss
created new optimizer with LR 7.8125e-06
Epoch 21/64
----------
0
24000
48000
72000
96000
120000
144000
168000
192000
216000
240000
264000
288000
Train_losses: [0.2485448569059372, 0.23860414326190948, 0.23417140543460846, 0.23109398782253265, 0.2286403626203537, 0.2266765534877777, 0.22503556311130524, 0.22336895763874054, 0.22203819453716278, 0.22045648097991943, 0.21903084218502045, 0.21764634549617767, 0.21622450649738312, 0.20988605916500092, 0.20717333257198334, 0.20503060519695282, 0.2030644416809082, 0.1974671334028244, 0.19292093813419342, 0.19036108255386353, 0.18897560238838196, 0.1882428675889969]
 34%|███▍      | 22/65 [53:04:57<106:05:30, 8882.10s/it]0
24000
Validation_losses: [0.2430315613746643, 0.23668794333934784, 0.23673632740974426, 0.23185330629348755, 0.23108941316604614, 0.23243823647499084, 0.22902993857860565, 0.2607364356517792, 0.23197954893112183, 0.22878891229629517, 0.23141248524188995, 0.22894157469272614, 0.2295830398797989, 0.22708526253700256, 0.2305670976638794, 0.23122388124465942, 0.23493289947509766, 0.23303672671318054, 0.2354302704334259, 0.23636792600154877, 0.23725992441177368, 0.2382611483335495]
decay loss from 7.8125e-06 to 3.90625e-06 as not seeing improvement in val loss
created new optimizer with LR 3.90625e-06
Epoch 22/64
----------
0
24000
48000
72000
96000
120000
144000
168000
192000
216000
240000
264000
288000
Train_losses: [0.2485448569059372, 0.23860414326190948, 0.23417140543460846, 0.23109398782253265, 0.2286403626203537, 0.2266765534877777, 0.22503556311130524, 0.22336895763874054, 0.22203819453716278, 0.22045648097991943, 0.21903084218502045, 0.21764634549617767, 0.21622450649738312, 0.20988605916500092, 0.20717333257198334, 0.20503060519695282, 0.2030644416809082, 0.1974671334028244, 0.19292093813419342, 0.19036108255386353, 0.18897560238838196, 0.1882428675889969, 0.18773753941059113]
 35%|███▌      | 23/65 [55:35:43<104:11:58, 8931.40s/it]0
24000
Validation_losses: [0.2430315613746643, 0.23668794333934784, 0.23673632740974426, 0.23185330629348755, 0.23108941316604614, 0.23243823647499084, 0.22902993857860565, 0.2607364356517792, 0.23197954893112183, 0.22878891229629517, 0.23141248524188995, 0.22894157469272614, 0.2295830398797989, 0.22708526253700256, 0.2305670976638794, 0.23122388124465942, 0.23493289947509766, 0.23303672671318054, 0.2354302704334259, 0.23636792600154877, 0.23725992441177368, 0.2382611483335495, 0.2379290908575058]
decay loss from 3.90625e-06 to 1.953125e-06 as not seeing improvement in val loss
created new optimizer with LR 1.953125e-06
Epoch 23/64
----------
0
24000
48000
72000
96000
120000
144000
168000
192000
216000
240000
264000
288000
Train_losses: [0.2485448569059372, 0.23860414326190948, 0.23417140543460846, 0.23109398782253265, 0.2286403626203537, 0.2266765534877777, 0.22503556311130524, 0.22336895763874054, 0.22203819453716278, 0.22045648097991943, 0.21903084218502045, 0.21764634549617767, 0.21622450649738312, 0.20988605916500092, 0.20717333257198334, 0.20503060519695282, 0.2030644416809082, 0.1974671334028244, 0.19292093813419342, 0.19036108255386353, 0.18897560238838196, 0.1882428675889969, 0.18773753941059113, 0.18767648935317993]
0
24000
Validation_losses: [0.2430315613746643, 0.23668794333934784, 0.23673632740974426, 0.23185330629348755, 0.23108941316604614, 0.23243823647499084, 0.22902993857860565, 0.2607364356517792, 0.23197954893112183, 0.22878891229629517, 0.23141248524188995, 0.22894157469272614, 0.2295830398797989, 0.22708526253700256, 0.2305670976638794, 0.23122388124465942, 0.23493289947509766, 0.23303672671318054, 0.2354302704334259, 0.23636792600154877, 0.23725992441177368, 0.2382611483335495, 0.2379290908575058, 0.23804570734500885]
decay loss from 1.953125e-06 to 9.765625e-07 as not seeing improvement in val loss
created new optimizer with LR 9.765625e-07
no improvement in 10 epochs, break
Training complete in 3487m 28s
saving
13
batch_size 48

